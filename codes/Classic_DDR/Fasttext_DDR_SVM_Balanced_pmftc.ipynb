{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HUGmxyH2iUc",
        "outputId": "a7a7afd9-de26-4043-c053-911259a4baf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 47.1 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394490 sha256=22601cdb059be6539a534bf18fc5fbd31b9f7e6770b933bef420eb7db641b84a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=153934 sha256=478c2d8b873917ad0a286f687ae242f9ce62a99f88d6c1b91fbf1e41f7c09088\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlrd\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 2.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed xlrd-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "#from __future__ import unicode_literals\n",
        "from hazm import *\n",
        "!pip install xlrd -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxXbMia-OyBO",
        "outputId": "7152a619-3809-49e0-9a37-4ac1fe7c7649"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler \n",
        "from imblearn.datasets import make_imbalance\n",
        "\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from nltk.stem import *\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wkSnlJKL2eyD"
      },
      "outputs": [],
      "source": [
        "ran=random.sample(range(1000, 100000), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwW_8VWMKQ2d",
        "outputId": "d2d62b69-757a-495d-e60a-b45e31a465d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AFf7CN7mUVul"
      },
      "outputs": [],
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    \n",
        "    # Remove URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Remove non character \n",
        "    text=re.sub(r'[\\W_]+',' ', text)\n",
        "    text=deEmojify(text)\n",
        "\n",
        "    # Remove  number\n",
        "    text = re.sub(\" \\d+\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"just\", \" \", text)\n",
        "    text = re.sub(\"right\", \" \", text)\n",
        "    '''\n",
        "\n",
        "\n",
        "    output = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n",
        "    text = output.rstrip()\n",
        "\n",
        "    persian = ['۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹'];\n",
        "    for u in persian:\n",
        "        text = re.sub(u, \"\", text);\n",
        "\n",
        "  \n",
        "    \n",
        "    #\n",
        "    #text = re.sub(\"کرونا\", \" \", text)\n",
        "    #text = re.sub(\"ویروس\", \" \", text)\n",
        "  \n",
        "    '''\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R83W92I8x0vw",
        "outputId": "1dea56b9-fd19-4ed4-8c0a-2341b2efebe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 3930, done.\u001b[K\n",
            "remote: Counting objects: 100% (943/943), done.\u001b[K\n",
            "remote: Compressing objects: 100% (137/137), done.\u001b[K\n",
            "remote: Total 3930 (delta 854), reused 806 (delta 806), pack-reused 2987\u001b[K\n",
            "Receiving objects: 100% (3930/3930), 8.24 MiB | 15.95 MiB/s, done.\n",
            "Resolving deltas: 100% (2505/2505), done.\n",
            "/content/fastText\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/fastText\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (57.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fasttext==0.9.2) (1.21.6)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp37-cp37m-linux_x86_64.whl size=3156178 sha256=f93e07689fd7eaa8bed5571e2df412457fbd3bed55a654b50da7e80297c9a082\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-4qqrn73y/wheels/22/04/6e/b3aba25c1a5845898b5871a0df37c2126cb0cc9326ad0c08e7\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.10.1\n",
            "/content\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "ft = fasttext.load_model('./drive/MyDrive/cc.fa.300.bin')\n",
        "ft.get_dimension()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lo6a3FjSTKrv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Tue Aug 16 14:43:05 2022\n",
        "\n",
        "@author: behnam\n",
        "\"\"\"\n",
        "\n",
        "def flat_list(a):\n",
        "    aa=[]\n",
        "    for j in range(len(a)):\n",
        "        \n",
        "        aa=aa+a[j].split(',')\n",
        "    return aa    \n",
        "        \n",
        "def text_preprocessing2(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    \n",
        "    # Remove URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"&\\S+\", \"\", text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    return text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YoHIaYkntpaS"
      },
      "outputs": [],
      "source": [
        "def set_reproducible(seed):\n",
        "  \n",
        "\n",
        "  os.environ['PYTHONHASHSEED'] = str(0)\n",
        "  # For working on GPUs from \"TensorFlow Determinism\"\n",
        "  os.environ['CUDA_VISBLE_DEVICE'] = ''\n",
        "  #np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WZpA2vCTBvoW"
      },
      "outputs": [],
      "source": [
        "stop_words=[]\n",
        "with open('./drive/MyDrive/stop-words.txt') as file:\n",
        "    for line in file:\n",
        "      stop_words.append(line.rstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "WukUW7uHMFe8"
      },
      "outputs": [],
      "source": [
        "pmfd=pd.read_excel('./drive/MyDrive/balanced_pmftc.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "tPz_30j_iF0G"
      },
      "outputs": [],
      "source": [
        "foundation='Qeirat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "mAQD3Sh0fvLR"
      },
      "outputs": [],
      "source": [
        "\n",
        "foundation_annot_inClass=pmfd[pmfd['Label']==foundation]\n",
        "foundation_annot_outClass=pmfd[pmfd['Label']!=foundation]\n",
        "x_inClass=foundation_annot_inClass['Tweet']\n",
        "x_outClass=foundation_annot_outClass['Tweet']\n",
        "y_inClass=len(x_inClass)*[1]\n",
        "y_outClass=len(x_outClass)*[0]\n",
        "x=x_inClass.tolist()+x_outClass.tolist()\n",
        "y=y_inClass+y_outClass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deGCH3X1rZj1",
        "outputId": "f874ea7d-c790-421d-c3ce-d5353679ec11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({0: 7500, 1: 500})\n",
            "Resampled dataset shape Counter({0: 500, 1: 500})\n"
          ]
        }
      ],
      "source": [
        "set_reproducible(1359)\n",
        "rus = RandomUnderSampler(random_state=42)\n",
        "#rus = RandomUnderSampler(sampling_strategy={0: 500, 1: 500 },random_state=42)\n",
        "x_res, y_res = rus.fit_resample(np.asarray(x).reshape(-1, 1),y)\n",
        "\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "print('Resampled dataset shape %s' % Counter(y_res))\n",
        "\n",
        "x_res=x_res.reshape(len(x_res))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "id": "LeGxh78hoIIS"
      },
      "outputs": [],
      "source": [
        "care_words=['حمایت','شفقت','همدلی','مهربانی','آسیب','ظلم','رنج','ستم']\n",
        "equality_words=['برابری','مساوات','عدالت','انصاف','حق','سوگیری','پیش داوری','تبعیض']\n",
        "proportionality_words=['رانت','دروغ','ریا','خیانت','کیفر','لیاقت','تناسب','شایستگی']\n",
        "loyalty_words=['وفاداری','پایبندی','عهد','وطن','خیانت','دشمن','معاند','دسیسه']\n",
        "authority_words=['شورش','اغتشاش','تمرد','برانداز','رهبر','رسم','احترام','اطاعت']\n",
        "purity_words=['آلوده','فساد','گناه','کثیف','رستگار','عصمت','تقدس','پاکی']\n",
        "liberty_words=['ترس','گرفتار','جبر','اسارت','اختیار','نجات','رهایی','آزادی']\n",
        "qeirat_words=['حیثیت','حیا','آبرو','نامرد','تعصب','شرف','ناموس','غیرت']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "SV4-j4tYx0PK"
      },
      "outputs": [],
      "source": [
        "def foundation_average_vector(word_list):\n",
        "  av=[]\n",
        "  for k in word_list:\n",
        "    av.append(ft.get_word_vector(k))\n",
        "  return np.average(np.asarray(av),axis=0)  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "9bksMw1SQSR2"
      },
      "outputs": [],
      "source": [
        "care_Average_Vector=foundation_average_vector(care_words)\n",
        "equality_Average_Vector=foundation_average_vector(equality_words)\n",
        "proportionality_Average_Vector=foundation_average_vector(proportionality_words)\n",
        "loyalty_Average_Vector=foundation_average_vector(loyalty_words)\n",
        "authority_Average_Vector=foundation_average_vector(authority_words)\n",
        "purity_Average_Vector=foundation_average_vector(purity_words)\n",
        "liberty_Average_Vector=foundation_average_vector(liberty_words)\n",
        "qeirat_Average_Vector=foundation_average_vector(qeirat_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NzbPJszDehn",
        "outputId": "b21c45b6-1e2b-4170-d7da-5a76e5aa1ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:00<00:00, 1054.23it/s]\n"
          ]
        }
      ],
      "source": [
        "word_seq=[]\n",
        "sw_removed_texts = []\n",
        "for doc in tqdm(x_res):\n",
        "  tokens = word_tokenize(doc)\n",
        "  filtered = [word for word in tokens if word not in stop_words] \n",
        "  #sw_removed_texts.append(\" \".join(filtered))\n",
        "  word_seq.append(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "fTkdh9v0Cg7X"
      },
      "outputs": [],
      "source": [
        "def tweet_moral_loading(tweet_tokens):\n",
        "  t=[ft.get_word_vector(k) for k in tweet_tokens]  #bert embedding of each tokens\n",
        "  t_av=np.average(np.asarray(t),axis=0)\n",
        "  res=[1-cosine(t_av,care_Average_Vector),1-cosine(t_av,equality_Average_Vector),1-cosine(t_av,proportionality_Average_Vector),1-cosine(t_av,loyalty_Average_Vector),\n",
        "   1-cosine(t_av,authority_Average_Vector),1-cosine(t_av,purity_Average_Vector),1-cosine(t_av,liberty_Average_Vector),\n",
        "   1-cosine(t_av,qeirat_Average_Vector)]\n",
        "  return np.asarray(res) \n",
        "                   \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inNYjT9fNQXK",
        "outputId": "a7c578fa-4b91-4600-c375-8a601b65244b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/lib/function_base.py:380: RuntimeWarning: Mean of empty slice.\n",
            "  avg = a.mean(axis)\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ],
      "source": [
        "feat_x=np.asarray(list(map(tweet_moral_loading,word_seq)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "I11HaEJjXv8I"
      },
      "outputs": [],
      "source": [
        "#x_train, x_test, y_train, y_test = train_test_split(feat_x,y_res, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AY4HkOdlYXDG",
        "outputId": "1f2b1197-18a6-43da-ae00-78c9b16ae761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.86 0.91 0.87 0.95 0.9  0.99 0.9  0.94 0.91 0.9 ]\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "clf = svm.SVC(kernel='rbf', C=1)\n",
        "scores = cross_val_score(clf,feat_x,y_res, cv=10)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gJbiMOla54J",
        "outputId": "3ff077a5-aa7f-4e0a-a022-955c483bc0ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qeirat\n",
            "0.913\n",
            "0.036345563690772485\n"
          ]
        }
      ],
      "source": [
        "print(foundation)\n",
        "print(np.mean(scores))\n",
        "print(np.std(scores))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "XGam6mduZf7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a71c4b30-7c86-4ffa-992f-6921ddaa01fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1:\n",
            "0.9116252836122716\n",
            "0.027394676758590104\n",
            "precision:\n",
            "0.9276254186347355\n",
            "0.03880660428471198\n",
            "recall:\n",
            "0.8979999999999999\n",
            "0.04044749683231336\n"
          ]
        }
      ],
      "source": [
        "cv = StratifiedKFold(n_splits=10,shuffle=True, random_state=1359)      \n",
        "f = cross_val_score(clf, feat_x,y_res,scoring='f1', cv=cv)\n",
        "p = cross_val_score(clf, feat_x,y_res,scoring='precision', cv=cv)\n",
        "r = cross_val_score(clf, feat_x,y_res,scoring='recall', cv=cv)\n",
        "print('f1:')\n",
        "print(np.mean(f))\n",
        "print(np.std(f))\n",
        "print('precision:')\n",
        "print(np.mean(p))\n",
        "print(np.std(p))\n",
        "print('recall:')\n",
        "print(np.mean(r))\n",
        "print(np.std(r))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}