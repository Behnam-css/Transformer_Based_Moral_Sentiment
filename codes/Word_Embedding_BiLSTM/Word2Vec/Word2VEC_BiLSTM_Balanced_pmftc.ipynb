{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HUGmxyH2iUc",
        "outputId": "79df8a63-f98d-41cb-c70e-19a0e5969faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlrd\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.1 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed xlrd-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install xlrd -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q60-zMW-x2Je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c86e2546-3ee0-4425-c8b1-1cb6055d634e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 32.7 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 41.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394487 sha256=13e41c61965a29037f43465e2fec5fd811dc0609effe8da501e38a5013d4aa0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154703 sha256=595419886e77a32b5c5d0dff12fdd83bc2668bc27197a21902b8c1b320989928\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm\n",
        "#from __future__ import unicode_literals\n",
        "from hazm import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nxXbMia-OyBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32611fa1-6e78-4d79-b1f2-b512b4986340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler \n",
        "from imblearn.datasets import make_imbalance\n",
        "\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from nltk.stem import *\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional,Lambda\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "from tensorflow.keras.losses import cosine_similarity\n",
        "\n",
        "\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wkSnlJKL2eyD"
      },
      "outputs": [],
      "source": [
        "ran=random.sample(range(1000, 100000), 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dwW_8VWMKQ2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "534aad04-111e-4732-b0b2-886ad64c84b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AFf7CN7mUVul"
      },
      "outputs": [],
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    \n",
        "    # Remove URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Remove non character \n",
        "    text=re.sub(r'[\\W_]+',' ', text)\n",
        "    text=deEmojify(text)\n",
        "\n",
        "    # Remove  number\n",
        "    text = re.sub(\" \\d+\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"just\", \" \", text)\n",
        "    text = re.sub(\"right\", \" \", text)\n",
        "    '''\n",
        "\n",
        "\n",
        "    output = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n",
        "    text = output.rstrip()\n",
        "\n",
        "    persian = ['۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹'];\n",
        "    for u in persian:\n",
        "        text = re.sub(u, \"\", text);\n",
        "\n",
        "  \n",
        "    \n",
        "    #\n",
        "    #text = re.sub(\"کرونا\", \" \", text)\n",
        "    #text = re.sub(\"ویروس\", \" \", text)\n",
        "  \n",
        "    '''\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Wmq1klOxyTee"
      },
      "outputs": [],
      "source": [
        "stop_words=[]\n",
        "with open('./drive/MyDrive/stop-words.txt') as file:\n",
        "    for line in file:\n",
        "      stop_words.append(line.rstrip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "R83W92I8x0vw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6KnhbtvF-YEe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Lo6a3FjSTKrv"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Created on Tue Aug 16 14:43:05 2022\n",
        "\n",
        "@author: behnam\n",
        "\"\"\"\n",
        "\n",
        "def flat_list(a):\n",
        "    aa=[]\n",
        "    for j in range(len(a)):\n",
        "        \n",
        "        aa=aa+a[j].split(',')\n",
        "    return aa    \n",
        "        \n",
        "def text_preprocessing2(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    \n",
        "    # Remove URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"&\\S+\", \"\", text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    return text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WukUW7uHMFe8"
      },
      "outputs": [],
      "source": [
        "pmfd=pd.read_excel('./drive/MyDrive/balanced_pmftc.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# http://vectors.nlpl.eu/repository/\n",
        "!wget  http://vectors.nlpl.eu/repository/20/61.zip\n",
        "!unzip 61.zip\n",
        "\n"
      ],
      "metadata": {
        "id": "pY2QLXS-AwCE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d170589b-e745-4f59-9556-2f9d2a4cb149"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-16 12:43:54--  http://vectors.nlpl.eu/repository/20/61.zip\n",
            "Resolving vectors.nlpl.eu (vectors.nlpl.eu)... 129.240.189.181\n",
            "Connecting to vectors.nlpl.eu (vectors.nlpl.eu)|129.240.189.181|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 730416332 (697M) [application/zip]\n",
            "Saving to: ‘61.zip’\n",
            "\n",
            "61.zip              100%[===================>] 696.58M  22.8MB/s    in 31s     \n",
            "\n",
            "2022-11-16 12:44:26 (22.4 MB/s) - ‘61.zip’ saved [730416332/730416332]\n",
            "\n",
            "Archive:  61.zip\n",
            "  inflating: LIST                    \n",
            "  inflating: meta.json               \n",
            "  inflating: model.bin               \n",
            "  inflating: model.txt               \n",
            "  inflating: README                  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "model_w2v=KeyedVectors.load_word2vec_format('model.bin', binary=True)\n",
        "vocab=list(model_w2v.vocab.keys())"
      ],
      "metadata": {
        "id": "HKQtlqWhDa-c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tPz_30j_iF0G"
      },
      "outputs": [],
      "source": [
        "foundation='Proportionality'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mAQD3Sh0fvLR"
      },
      "outputs": [],
      "source": [
        "\n",
        "foundation_annot_inClass=pmfd[pmfd['Label']==foundation]\n",
        "foundation_annot_outClass=pmfd[pmfd['Label']!=foundation]\n",
        "x_inClass=foundation_annot_inClass['Tweet']\n",
        "x_outClass=foundation_annot_outClass['Tweet']\n",
        "y_inClass=len(x_inClass)*[1]\n",
        "y_outClass=len(x_outClass)*[0]\n",
        "x=x_inClass.tolist()+x_outClass.tolist()\n",
        "y=y_inClass+y_outClass\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "deGCH3X1rZj1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f85b865-a266-476f-b647-4719d58c178c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({0: 7500, 1: 500})\n",
            "Resampled dataset shape Counter({0: 500, 1: 500})\n"
          ]
        }
      ],
      "source": [
        "rus = RandomUnderSampler(random_state=42)\n",
        "#rus = RandomUnderSampler(sampling_strategy={0: 500, 1: 500 },random_state=42)\n",
        "x_res, y_res = rus.fit_resample(np.asarray(x).reshape(-1, 1),y)\n",
        "\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "print('Resampled dataset shape %s' % Counter(y_res))\n",
        "\n",
        "x_res=x_res.reshape(len(x_res))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ge0WwvKY6RKN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1NzbPJszDehn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78c3ddc6-4757-47aa-a9e3-8de29618f195"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 931.52it/s]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "word_seq=[]\n",
        "sw_removed_texts = []\n",
        "for doc in tqdm(x_res):\n",
        "  tokens = word_tokenize(doc)\n",
        "  filtered = [word for word in tokens if word not in stop_words] \n",
        "  #sw_removed_texts.append(\" \".join(filtered))\n",
        "  word_seq.append(filtered)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "m_hOWcM8Iwrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b5ce902-6c97-48a6-d547-6dc9a58c3dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 921.72it/s]\n"
          ]
        }
      ],
      "source": [
        "max_seq_len=64\n",
        "MAX_NB_WORDS=100000\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "sw_removed_texts = []\n",
        "for doc in tqdm(x_res):\n",
        "  tokens = word_tokenize(doc)\n",
        "  filtered = [word for word in tokens if word not in stop_words]\n",
        "  sw_removed_texts.append(\" \".join(filtered))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(sw_removed_texts)  \n",
        "word_seq = tokenizer.texts_to_sequences(sw_removed_texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "word_seq= pad_sequences(word_seq, maxlen=max_seq_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def all_unique_words_in_dataset():\n",
        "  all_words=[]\n",
        "  for doc in tqdm(x_res):\n",
        "    tokens = word_tokenize(doc)\n",
        "    filtered = [word for word in tokens if word not in stop_words]\n",
        "    all_words=all_words+filtered\n",
        "  return set(all_words)\n",
        "\n",
        "\n",
        "all_words=all_unique_words_in_dataset()\n",
        "len(all_words)"
      ],
      "metadata": {
        "id": "-9ZZX9mx6YDy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f397cb17-94f4-466b-f61d-e08548ea4605"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 869.06it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6161"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_vectors={}\n",
        "for w in tqdm(all_words):\n",
        "\n",
        "  if w in vocab:\n",
        "   embeddings_vectors[w] = model_w2v[w]\n",
        "  else:\n",
        "   embeddings_vectors[w] = model_w2v['خنثی'] "
      ],
      "metadata": {
        "id": "b1BSS5dS8eQK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "469eb9be-861a-4fc4-907e-813ed5a2adcb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 6161/6161 [00:30<00:00, 205.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9jvuY2pjF7RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90c6746e-fb19-4320-e1cf-7415ec84edc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6162\n",
            "number of null word embeddings: 13\n"
          ]
        }
      ],
      "source": [
        "embed_dim=100\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
        "print(nb_words)\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i >= nb_words:\n",
        "     continue\n",
        "  embedding_vector = embeddings_vectors.get(word)\n",
        "  \n",
        "  if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "     embedding_matrix[i] = embedding_vector\n",
        "  else:\n",
        "     words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YoHIaYkntpaS"
      },
      "outputs": [],
      "source": [
        "def set_reproducible(seed):\n",
        "  \n",
        "\n",
        "  os.environ['PYTHONHASHSEED'] = str(0)\n",
        "  # For working on GPUs from \"TensorFlow Determinism\"\n",
        "  os.environ['CUDA_VISBLE_DEVICE'] = ''\n",
        "  #np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wZbZx08WgAg2"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n",
        "  model.add(tf.keras.layers.LayerNormalization(axis=1)) \n",
        "  model.add(Bidirectional(LSTM(16, return_sequences= False)))\n",
        "  #model.add(Bidirectional(LSTM(8, return_sequences= False)))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "  #model.summary()\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NgYTujB2W1hV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "1875ccdc-83d2-4a17-abc0-dd4c3c3a3192"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nf_binary_all=[]\\nf_macro_all=[]\\nf_micro_all=[]\\np_all =[]\\nr_all=[]\\nfold = 0\\n\\n\\nfor k in ran:\\n\\n  fold+=1\\n  x_train_a, x_test_a, y_train_a, y_test_a =train_test_split(word_seq,np.asanyarray(y_res), test_size=0.2, random_state=k)\\n\\n  set_reproducible(k)\\n  model=get_model()\\n  es_callback = EarlyStopping(monitor=\\'val_loss\\', patience=3)\\n  history = model.fit(x_train_a, y_train_a, batch_size=32, epochs=200,verbose=1, validation_split=0.2, callbacks=[es_callback], shuffle=True)\\n\\n  yhat=model.predict(x_test_a, verbose=0)\\n\\n  yp=[]\\n  for k in range(len(yhat)):\\n    if yhat[k][0]>0.45:\\n      yp.append(1)\\n    else:\\n      yp.append(0)\\n\\n  print(classification_report(y_test_a,yp))   \\n  print(\\'F1-score for fold: %d :\\' %fold)\\n  f1_macro=f1_score(y_test_a,yp,average=\\'macro\\')\\n  f1_binary=f1_score(y_test_a,yp,average=\\'binary\\')\\n  f1_micro=f1_score(y_test_a,yp,average=\\'micro\\')\\n\\n  print(f1_binary)\\n  print(\\'-----\\') \\n  f_binary_all.append(f1_binary)\\n\\n  print(f1_macro)\\n  print(\\'-----\\') \\n  f_macro_all.append(f1_macro)\\n\\n  print(f1_micro)\\n  print(\\'-----\\') \\n  f_micro_all.append(f1_micro)\\n\\n  print(\\'Recall-score for fold: %d :\\' %fold)\\n  r=recall_score(y_test_a,yp)\\n  print(r)\\n  print(\\'-----\\')\\n  print(\\'Precision-score for fold: %d :\\' %fold)\\n  p=precision_score(y_test_a,yp)\\n  print(p)\\n  print(\\'-----\\')\\n  print(\\'----------------------------------------------\\')\\n\\n  r_all.append(r)\\n  p_all.append(p)\\n \\n\\nprint(\"Average of F1_binary_Score = {}\".format(np.mean(f_binary_all)))\\nprint(\"Standard Deviation of F1_binary_Score = {}\".format(np.std(f_binary_all)))\\nprint(\\'------\\')\\n\\nprint(\"Average of F1_macro_Score = {}\".format(np.mean(f_macro_all)))\\nprint(\"Standard Deviation of F1_macro_Score = {}\".format(np.std(f_macro_all)))\\nprint(\\'------\\')\\n\\nprint(\"Average of F1_micro_Score = {}\".format(np.mean(f_micro_all)))\\nprint(\"Standard Deviation of F1_micro_Score = {}\".format(np.std(f_micro_all)))\\nprint(\\'------\\')\\n\\nprint(\"Average of Recall_Score = {}\".format(np.mean(r_all)))\\nprint(\"Standard Deviation of Recall_Score = {}\".format(np.std(r_all)))\\nprint(\\'------\\')\\n\\nprint(\"Average of Precision_Score = {}\".format(np.mean(p_all)))\\nprint(\"Standard Deviation of Precision_Score = {}\".format(np.std(p_all)))\\nprint(\\'------\\')\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "'''\n",
        "f_binary_all=[]\n",
        "f_macro_all=[]\n",
        "f_micro_all=[]\n",
        "p_all =[]\n",
        "r_all=[]\n",
        "fold = 0\n",
        "\n",
        "\n",
        "for k in ran:\n",
        "\n",
        "  fold+=1\n",
        "  x_train_a, x_test_a, y_train_a, y_test_a =train_test_split(word_seq,np.asanyarray(y_res), test_size=0.2, random_state=k)\n",
        "\n",
        "  set_reproducible(k)\n",
        "  model=get_model()\n",
        "  es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "  history = model.fit(x_train_a, y_train_a, batch_size=32, epochs=200,verbose=1, validation_split=0.2, callbacks=[es_callback], shuffle=True)\n",
        "\n",
        "  yhat=model.predict(x_test_a, verbose=0)\n",
        "\n",
        "  yp=[]\n",
        "  for k in range(len(yhat)):\n",
        "    if yhat[k][0]>0.45:\n",
        "      yp.append(1)\n",
        "    else:\n",
        "      yp.append(0)\n",
        "\n",
        "  print(classification_report(y_test_a,yp))   \n",
        "  print('F1-score for fold: %d :' %fold)\n",
        "  f1_macro=f1_score(y_test_a,yp,average='macro')\n",
        "  f1_binary=f1_score(y_test_a,yp,average='binary')\n",
        "  f1_micro=f1_score(y_test_a,yp,average='micro')\n",
        "\n",
        "  print(f1_binary)\n",
        "  print('-----') \n",
        "  f_binary_all.append(f1_binary)\n",
        "\n",
        "  print(f1_macro)\n",
        "  print('-----') \n",
        "  f_macro_all.append(f1_macro)\n",
        "\n",
        "  print(f1_micro)\n",
        "  print('-----') \n",
        "  f_micro_all.append(f1_micro)\n",
        "\n",
        "  print('Recall-score for fold: %d :' %fold)\n",
        "  r=recall_score(y_test_a,yp)\n",
        "  print(r)\n",
        "  print('-----')\n",
        "  print('Precision-score for fold: %d :' %fold)\n",
        "  p=precision_score(y_test_a,yp)\n",
        "  print(p)\n",
        "  print('-----')\n",
        "  print('----------------------------------------------')\n",
        "\n",
        "  r_all.append(r)\n",
        "  p_all.append(p)\n",
        " \n",
        "\n",
        "print(\"Average of F1_binary_Score = {}\".format(np.mean(f_binary_all)))\n",
        "print(\"Standard Deviation of F1_binary_Score = {}\".format(np.std(f_binary_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of F1_macro_Score = {}\".format(np.mean(f_macro_all)))\n",
        "print(\"Standard Deviation of F1_macro_Score = {}\".format(np.std(f_macro_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of F1_micro_Score = {}\".format(np.mean(f_micro_all)))\n",
        "print(\"Standard Deviation of F1_micro_Score = {}\".format(np.std(f_micro_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of Recall_Score = {}\".format(np.mean(r_all)))\n",
        "print(\"Standard Deviation of Recall_Score = {}\".format(np.std(r_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of Precision_Score = {}\".format(np.mean(p_all)))\n",
        "print(\"Standard Deviation of Precision_Score = {}\".format(np.std(p_all)))\n",
        "print('------')\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bh5zvqWPU05v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7af4dff-a7a6-4cda-b93d-8b0dbcad77db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "57/57 [==============================] - 8s 64ms/step - loss: 1.0572 - accuracy: 0.4933 - val_loss: 0.6861 - val_accuracy: 0.4900\n",
            "Epoch 2/200\n",
            "52/57 [==========================>...] - ETA: 0s - loss: 0.7299 - accuracy: 0.4243"
          ]
        }
      ],
      "source": [
        "\n",
        "split_num=10\n",
        "kf = StratifiedKFold(n_splits=split_num, shuffle=True, random_state=2022)\n",
        "\n",
        "f_binary_all=[]\n",
        "f_macro_all=[]\n",
        "p_all =[]\n",
        "r_all=[]\n",
        "fold = 0\n",
        "\n",
        "for train_idx, val_idx in tqdm(kf.split(word_seq,np.asarray(y_res))):\n",
        "\n",
        "   \n",
        "\n",
        "    x_train_f = word_seq[train_idx]\n",
        "    y_train_f = np.asarray(y_res)[train_idx]\n",
        "    x_val_f = word_seq[val_idx]\n",
        "    y_val_f =np.asarray(y_res)[val_idx]\n",
        "\n",
        "    set_reproducible(ran[fold])\n",
        "    model=get_model()\n",
        "    es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "    model.fit(x_train_f, y_train_f, batch_size=16, epochs=200,verbose=1, callbacks=[es_callback],validation_data=(x_val_f, y_val_f), shuffle=False)\n",
        "    #model.fit(x_train_f, y_train_f, batch_size=16, epochs=200,verbose=0,validation_data=(x_val_f, y_val_f), shuffle=False )\n",
        "\n",
        "    #model.fit(x_train_f, y_train_f, batch_size=32, epochs=200,verbose=0, callbacks=[es_callback],validation_split=0.1)\n",
        "\n",
        "\n",
        "\n",
        "    yhat=model.predict(x_val_f, verbose=0)\n",
        "\n",
        "    yp=[]\n",
        "    for k in range(len(yhat)):\n",
        "      if yhat[k][0]>0.45:\n",
        "        yp.append(1)\n",
        "      else:\n",
        "        yp.append(0)\n",
        "\n",
        "    print(classification_report(y_val_f,yp)) \n",
        "\n",
        "    print('F1_binary-score for fold: %d :' %fold)\n",
        "    f1_binary=f1_score(y_val_f,yp,average='binary')\n",
        "    print(f1_binary)\n",
        "    print('-----')\n",
        "\n",
        "    print('F1_macro-score for fold: %d :' %fold)\n",
        "    f1_macro=f1_score(y_val_f,yp,average='macro')\n",
        "    print(f1_macro)\n",
        "    print('-----')\n",
        "\n",
        "    print('Recall-score for fold: %d :' %fold)\n",
        "    r=recall_score(y_val_f,yp)\n",
        "    print(r)\n",
        "    print('-----')\n",
        "    print('Precision-score for fold: %d :' %fold)\n",
        "    p=precision_score(y_val_f,yp)\n",
        "    print(p)\n",
        "    print('-----')\n",
        "    print('----------------------------------------------')\n",
        "    f_binary_all.append(f1_binary)\n",
        "    f_macro_all.append(f1_macro)\n",
        "    r_all.append(r)\n",
        "    p_all.append(p)\n",
        "    fold+=1\n",
        "\n",
        "\n",
        "\n",
        "print(\"Average of F1_binary_Score = {}\".format(np.mean(f_binary_all)))\n",
        "print(\"Standard Deviation of F1_binary_Score = {}\".format(np.std(f_binary_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of F1_macro_Score = {}\".format(np.mean(f_macro_all)))\n",
        "print(\"Standard Deviation of F1_binary_Score = {}\".format(np.std(f_macro_all)))\n",
        "print('------')\n",
        "\n",
        "\n",
        "print(\"Average of Recall_Score = {}\".format(np.mean(r_all)))\n",
        "print(\"Standard Deviation of Recall_Score = {}\".format(np.std(r_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of Precision_Score = {}\".format(np.mean(p_all)))\n",
        "print(\"Standard Deviation of Precision_Score = {}\".format(np.std(p_all)))\n",
        "print('------')\n",
        "\n",
        "\n",
        "\n",
        "print(np.mean([k for k  in f_macro_all if k>0.79 ]))\n",
        "print(f_binary_all)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "govFKSyEedUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}