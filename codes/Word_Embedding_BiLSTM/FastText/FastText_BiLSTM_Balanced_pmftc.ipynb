{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install hazm\n",
        "#from __future__ import unicode_literals\n",
        "from hazm import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTkdh9v0Cg7X",
        "outputId": "7d95261f-8c17-4afc-acda-d94e408a036c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 44.4 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=a8150c997238ab5c9f9367d63dcc0cca4f55a6784f656c6daebf6c2bcf6944ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154650 sha256=2b47b95044c2f8daea4d6efeacffcb0377b9f00ab0eee03323db50cca5876765\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.7\n",
            "    Uninstalling nltk-3.7:\n",
            "      Successfully uninstalled nltk-3.7\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xlrd -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HUGmxyH2iUc",
        "outputId": "5e59e442-2bdb-4621-92c0-b4e1cba5dbf3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.7/dist-packages (1.1.0)\n",
            "Collecting xlrd\n",
            "  Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 3.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: xlrd\n",
            "  Attempting uninstall: xlrd\n",
            "    Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "Successfully installed xlrd-2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxxGUdO8L9fD",
        "outputId": "aad80f2c-c920-4707-c7f0-1d4bc55ab805"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.9.2)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 588.3 MB 6.0 kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.27.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.1)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.10.26-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (4.1.1)\n",
            "Collecting tensorboard<2.12,>=2.11\n",
            "  Downloading tensorboard-2.11.0-py3-none-any.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
            "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
            "\u001b[K     |████████████████████████████████| 439 kB 54.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Collecting keras<2.12,>=2.11.0\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.50.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.3)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.14.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, flatbuffers, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 1.12\n",
            "    Uninstalling flatbuffers-1.12:\n",
            "      Successfully uninstalled flatbuffers-1.12\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed flatbuffers-22.10.26 keras-2.11.0 tensorboard-2.11.0 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxXbMia-OyBO",
        "outputId": "6a512991-d2f2-44e1-c20c-10d1bcd5befb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import random\n",
        "import time\n",
        "from collections import Counter\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "\n",
        "from imblearn.under_sampling import RandomUnderSampler \n",
        "from imblearn.datasets import make_imbalance\n",
        "\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import RegexpTokenizer \n",
        "from nltk.stem import *\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "\n",
        "import keras\n",
        "from keras import optimizers\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from keras import regularizers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Flatten\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional,Lambda\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "from scipy.spatial.distance import cosine\n",
        "from tensorflow.keras.losses import cosine_similarity\n",
        "\n",
        "\n",
        "from keras.utils.data_utils import pad_sequences\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "import os\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ran=random.sample(range(1000, 100000), 10)"
      ],
      "metadata": {
        "id": "wkSnlJKL2eyD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dwW_8VWMKQ2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95c2e1b-a3d9-45b6-ec54-39ba71ced897"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AFf7CN7mUVul"
      },
      "outputs": [],
      "source": [
        "def deEmojify(text):\n",
        "    regrex_pattern = re.compile(pattern = \"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags = re.UNICODE)\n",
        "    return regrex_pattern.sub(r'',text)\n",
        "\n",
        "\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "    \n",
        "    # Remove URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    # Remove non character \n",
        "    text=re.sub(r'[\\W_]+',' ', text)\n",
        "    text=deEmojify(text)\n",
        "\n",
        "    # Remove  number\n",
        "    text = re.sub(\" \\d+\", \" \", text)\n",
        "\n",
        "    text = re.sub(\"just\", \" \", text)\n",
        "    text = re.sub(\"right\", \" \", text)\n",
        "    '''\n",
        "\n",
        "\n",
        "    output = re.sub(r'\\s*[A-Za-z]+\\b', '' , text)\n",
        "    text = output.rstrip()\n",
        "\n",
        "    persian = ['۰', '۱', '۲', '۳', '۴', '۵', '۶', '۷', '۸', '۹'];\n",
        "    for u in persian:\n",
        "        text = re.sub(u, \"\", text);\n",
        "\n",
        "  \n",
        "    \n",
        "    #\n",
        "    #text = re.sub(\"کرونا\", \" \", text)\n",
        "    #text = re.sub(\"ویروس\", \" \", text)\n",
        "  \n",
        "    '''\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "ft = fasttext.load_model('./drive/MyDrive/cc.fa.300.bin')\n",
        "ft.get_dimension()\n",
        "\n",
        "embeddings_index = {}\n",
        "embeddings_vectors={}\n",
        "for w in ft.words:\n",
        "  embeddings_vectors[w] = ft.get_word_vector(w)\n",
        "\n",
        "\n",
        "del ft\n",
        "\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "try:\n",
        "\tgeeky_file = open('persian_embeddings_vectors_fasttext', 'wb')\n",
        "\tpickle.dump(embeddings_vectors, geeky_file)\n",
        "\tgeeky_file.close()\n",
        "\n",
        "except:\n",
        "\tprint(\"Something went wrong\")  \n",
        " \n",
        "!cp persian_embeddings_vectors_fasttext ./drive/MyDrive  \n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "R83W92I8x0vw",
        "outputId": "609497ca-cd2b-42f9-c631-9326741732b1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n!git clone https://github.com/facebookresearch/fastText.git\\n%cd fastText\\n!pip install .\\n%cd ..\\n\\nimport fasttext\\nimport fasttext.util\\n\\nft = fasttext.load_model(\\'./drive/MyDrive/cc.fa.300.bin\\')\\nft.get_dimension()\\n\\nembeddings_index = {}\\nembeddings_vectors={}\\nfor w in ft.words:\\n  embeddings_vectors[w] = ft.get_word_vector(w)\\n\\n\\ndel ft\\n\\n\\nimport pickle\\n\\n\\ntry:\\n\\tgeeky_file = open(\\'persian_embeddings_vectors_fasttext\\', \\'wb\\')\\n\\tpickle.dump(embeddings_vectors, geeky_file)\\n\\tgeeky_file.close()\\n\\nexcept:\\n\\tprint(\"Something went wrong\")  \\n \\n!cp persian_embeddings_vectors_fasttext ./drive/MyDrive  \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open('./drive/MyDrive/persian_embeddings_vectors_fasttext','rb')\n",
        "embeddings_vectors= pickle.load(file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "JFWq7KppqDx6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "xls = pd.ExcelFile('./drive/MyDrive/MFD-final.xls')\n",
        "\n",
        "dic_data_Care_Virtue= pd.read_excel(xls,'Care_Virtue')['words'].to_list()\n",
        "dic_data_Care_Vice= pd.read_excel(xls, 'Care_Vice')['words'].to_list()\n",
        "care_words=dic_data_Care_Virtue+dic_data_Care_Vice\n",
        "\n",
        "dic_data_Fairness_Virtue= pd.read_excel(xls,'Fairness_Virtue')['words'].to_list()\n",
        "dic_data_Fairness_Vice= pd.read_excel(xls, 'Fairness_Vice')['words'].to_list()\n",
        "fairness_words=dic_data_Fairness_Virtue+dic_data_Fairness_Vice\n",
        "\n",
        "dic_data_Loyalty_Virtue= pd.read_excel(xls,'Loyalty_Virtue')['words'].to_list()\n",
        "dic_data_Loyalty_Vice= pd.read_excel(xls, 'Loyalty_Vice')['words'].to_list()\n",
        "loyalty_words=dic_data_Loyalty_Virtue+dic_data_Loyalty_Vice\n",
        "\n",
        "dic_data_Authority_Virtue= pd.read_excel(xls,'Authority_Virtue')['words'].to_list()\n",
        "dic_data_Authority_Vice= pd.read_excel(xls, 'Authority_Vice')['words'].to_list()\n",
        "authority_words=dic_data_Authority_Virtue+dic_data_Authority_Vice\n",
        "\n",
        "dic_data_Purity_Virtue= pd.read_excel(xls,'Purity_Virtue')['words'].to_list()\n",
        "dic_data_Purity_Vice= pd.read_excel(xls, 'Purity_Vice')['words'].to_list()\n",
        "purity_words=dic_data_Purity_Virtue+dic_data_Purity_Vice\n",
        "\n",
        "dic_data_Freedom_Virtue= pd.read_excel(xls,'Freedom_Virtue')['words'].to_list()\n",
        "dic_data_Freedom_Vice= pd.read_excel(xls, 'Freedom_Vice')['words'].to_list()\n",
        "freedom_words=dic_data_Freedom_Virtue+dic_data_Freedom_Vice\n",
        "\n",
        "dic_data_Qeirat_Virtue= pd.read_excel(xls,'Qeirat_Virtue')['words'].to_list()\n",
        "dic_data_Qeirat_Vice= pd.read_excel(xls, 'Qeirat_Vice')['words'].to_list()\n",
        "qeirat_words=dic_data_Qeirat_Virtue+dic_data_Qeirat_Vice\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "!git clone https://github.com/facebookresearch/fastText.git\n",
        "%cd fastText\n",
        "!pip install .\n",
        "%cd ..\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "\n",
        "ft = fasttext.load_model('./drive/MyDrive/cc.fa.300.bin')\n",
        "ft.get_dimension()\n",
        "\n",
        "\n",
        "def save_dic(dic_name):\n",
        "  fname='persian_mf_fasttext_'+variablename(dic_name)[0]\n",
        "  print(fname)\n",
        "  try:\n",
        "\n",
        "    with open(fname, 'wb') as handle:\n",
        "        pickle.dump(dic_name, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "\n",
        "  except:\n",
        "    print(\"Something went wrong\")\n",
        "\n",
        "def variablename(var):\n",
        " \n",
        "  return [tpl[0] for tpl in filter(lambda x: var is x[1], globals().items())]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "care_wv={}\n",
        "for w in care_words:\n",
        "  care_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "fairness_wv={}\n",
        "for w in fairness_words:\n",
        "  fairness_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "loyalty_wv={}\n",
        "for w in loyalty_words:\n",
        "  loyalty_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "authority_wv={}\n",
        "for w in authority_words:\n",
        "  authority_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "purity_wv={}\n",
        "for w in purity_words:\n",
        "  purity_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "freedom_wv={}\n",
        "for w in freedom_words:\n",
        "  freedom_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "qeirat_wv={}\n",
        "for w in qeirat_words:x=x_inClass.tolist()+x_outClass.tolist()\n",
        "  qeirat_wv[w] = ft.get_word_vector(w)\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "save_dic(care_wv)\n",
        "save_dic(fairness_wv)\n",
        "save_dic(loyalty_wv)\n",
        "save_dic(authority_wv)\n",
        "save_dic(purity_wv)\n",
        "save_dic(freedom_wv)\n",
        "save_dic(qeirat_wv)\n",
        "\n",
        "\n",
        "!cp persian_mf_fasttext_care_wv ./drive/MyDrive/\n",
        "!cp persian_mf_fasttext_fairness_wv ./drive/MyDrive/\n",
        "!cp persian_mf_fasttext_loyalty_wv ./drive/MyDrive/\n",
        "!cp persian_mf_fasttext_authority_wv ./drive/MyDrive/\n",
        "!cp persian_mf_fasttext_purity_wv ./drive/MyDrive/\n",
        "!cp persian_mf_fasttext_freedom_wv ./drive/MyDrive/\n",
        "!cp persian_mf_fasttext_qeirat_wv ./drive/MyDrive/\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "iR0bcWjeGWWg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "cd6e5571-065d-442e-e50f-27c2237a7b61"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nxls = pd.ExcelFile(\\'./drive/MyDrive/MFD-final.xls\\')\\n\\ndic_data_Care_Virtue= pd.read_excel(xls,\\'Care_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Care_Vice= pd.read_excel(xls, \\'Care_Vice\\')[\\'words\\'].to_list()\\ncare_words=dic_data_Care_Virtue+dic_data_Care_Vice\\n\\ndic_data_Fairness_Virtue= pd.read_excel(xls,\\'Fairness_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Fairness_Vice= pd.read_excel(xls, \\'Fairness_Vice\\')[\\'words\\'].to_list()\\nfairness_words=dic_data_Fairness_Virtue+dic_data_Fairness_Vice\\n\\ndic_data_Loyalty_Virtue= pd.read_excel(xls,\\'Loyalty_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Loyalty_Vice= pd.read_excel(xls, \\'Loyalty_Vice\\')[\\'words\\'].to_list()\\nloyalty_words=dic_data_Loyalty_Virtue+dic_data_Loyalty_Vice\\n\\ndic_data_Authority_Virtue= pd.read_excel(xls,\\'Authority_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Authority_Vice= pd.read_excel(xls, \\'Authority_Vice\\')[\\'words\\'].to_list()\\nauthority_words=dic_data_Authority_Virtue+dic_data_Authority_Vice\\n\\ndic_data_Purity_Virtue= pd.read_excel(xls,\\'Purity_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Purity_Vice= pd.read_excel(xls, \\'Purity_Vice\\')[\\'words\\'].to_list()\\npurity_words=dic_data_Purity_Virtue+dic_data_Purity_Vice\\n\\ndic_data_Freedom_Virtue= pd.read_excel(xls,\\'Freedom_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Freedom_Vice= pd.read_excel(xls, \\'Freedom_Vice\\')[\\'words\\'].to_list()\\nfreedom_words=dic_data_Freedom_Virtue+dic_data_Freedom_Vice\\n\\ndic_data_Qeirat_Virtue= pd.read_excel(xls,\\'Qeirat_Virtue\\')[\\'words\\'].to_list()\\ndic_data_Qeirat_Vice= pd.read_excel(xls, \\'Qeirat_Vice\\')[\\'words\\'].to_list()\\nqeirat_words=dic_data_Qeirat_Virtue+dic_data_Qeirat_Vice\\n\\n\\n\\n\\n\\n!git clone https://github.com/facebookresearch/fastText.git\\n%cd fastText\\n!pip install .\\n%cd ..\\n\\nimport fasttext\\nimport fasttext.util\\n\\nft = fasttext.load_model(\\'./drive/MyDrive/cc.fa.300.bin\\')\\nft.get_dimension()\\n\\n\\ndef save_dic(dic_name):\\n  fname=\\'persian_mf_fasttext_\\'+variablename(dic_name)[0]\\n  print(fname)\\n  try:\\n\\n    with open(fname, \\'wb\\') as handle:\\n        pickle.dump(dic_name, handle, protocol=pickle.HIGHEST_PROTOCOL)\\n\\n\\n\\n  except:\\n    print(\"Something went wrong\")\\n\\ndef variablename(var):\\n \\n  return [tpl[0] for tpl in filter(lambda x: var is x[1], globals().items())]\\n\\n\\n\\n\\ncare_wv={}\\nfor w in care_words:\\n  care_wv[w] = ft.get_word_vector(w)\\n\\nfairness_wv={}\\nfor w in fairness_words:\\n  fairness_wv[w] = ft.get_word_vector(w)\\n\\nloyalty_wv={}\\nfor w in loyalty_words:\\n  loyalty_wv[w] = ft.get_word_vector(w)\\n\\nauthority_wv={}\\nfor w in authority_words:\\n  authority_wv[w] = ft.get_word_vector(w)\\n\\npurity_wv={}\\nfor w in purity_words:\\n  purity_wv[w] = ft.get_word_vector(w)\\n\\nfreedom_wv={}\\nfor w in freedom_words:\\n  freedom_wv[w] = ft.get_word_vector(w)\\n\\nqeirat_wv={}\\nfor w in qeirat_words:x=x_inClass.tolist()+x_outClass.tolist()\\n  qeirat_wv[w] = ft.get_word_vector(w)\\n\\nimport pickle\\n\\n\\nsave_dic(care_wv)\\nsave_dic(fairness_wv)\\nsave_dic(loyalty_wv)\\nsave_dic(authority_wv)\\nsave_dic(purity_wv)\\nsave_dic(freedom_wv)\\nsave_dic(qeirat_wv)\\n\\n\\n!cp persian_mf_fasttext_care_wv ./drive/MyDrive/\\n!cp persian_mf_fasttext_fairness_wv ./drive/MyDrive/\\n!cp persian_mf_fasttext_loyalty_wv ./drive/MyDrive/\\n!cp persian_mf_fasttext_authority_wv ./drive/MyDrive/\\n!cp persian_mf_fasttext_purity_wv ./drive/MyDrive/\\n!cp persian_mf_fasttext_freedom_wv ./drive/MyDrive/\\n!cp persian_mf_fasttext_qeirat_wv ./drive/MyDrive/\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on Tue Aug 16 14:43:05 2022\n",
        "\n",
        "@author: behnam\n",
        "\"\"\"\n",
        "\n",
        "def flat_list(a):\n",
        "    aa=[]\n",
        "    for j in range(len(a)):\n",
        "        \n",
        "        aa=aa+a[j].split(',')\n",
        "    return aa    \n",
        "        \n",
        "def text_preprocessing2(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '', text)\n",
        "    \n",
        "    # Remove URL\n",
        "    text = re.sub(r\"http\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"#\\S+\", \"\", text)\n",
        "    \n",
        "    # Remove Hashtag\n",
        "    text = re.sub(r\"&\\S+\", \"\", text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    return text\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    \n",
        "    text=re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "Lo6a3FjSTKrv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pmfd=pd.read_excel('./drive/MyDrive/balanced_pmftc.xlsx')"
      ],
      "metadata": {
        "id": "WukUW7uHMFe8"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ff='Proportionality'"
      ],
      "metadata": {
        "id": "tPz_30j_iF0G"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "foundation_annot_inClass=pmfd[pmfd['Label']==ff]\n",
        "foundation_annot_outClass=pmfd[pmfd['Label']!=ff]\n",
        "x_inClass=foundation_annot_inClass['Tweet']\n",
        "x_outClass=foundation_annot_outClass['Tweet']\n",
        "y_inClass=len(x_inClass)*[1]\n",
        "y_outClass=len(x_outClass)*[0]\n",
        "x=x_inClass.tolist()+x_outClass.tolist()\n",
        "y=y_inClass+y_outClass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mAQD3Sh0fvLR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus = RandomUnderSampler(random_state=42)\n",
        "#rus = RandomUnderSampler(sampling_strategy={0: 500, 1: 500 },random_state=42)\n",
        "x_res, y_res = rus.fit_resample(np.asarray(x).reshape(-1, 1),y)\n",
        "\n",
        "print('Original dataset shape %s' % Counter(y))\n",
        "print('Resampled dataset shape %s' % Counter(y_res))\n",
        "\n",
        "x_res=x_res.reshape(len(x_res))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deGCH3X1rZj1",
        "outputId": "9028a422-a876-4243-93f5-630203606526"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original dataset shape Counter({0: 7500, 1: 500})\n",
            "Resampled dataset shape Counter({0: 500, 1: 500})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA61zbBY9kWN"
      },
      "source": [
        "Embedding layer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=[]\n",
        "with open('./drive/MyDrive/stop-words.txt') as file:\n",
        "    for line in file:\n",
        "      stop_words.append(line.rstrip())"
      ],
      "metadata": {
        "id": "WZpA2vCTBvoW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "\n",
        "word_seq=[]\n",
        "sw_removed_texts = []\n",
        "for doc in tqdm(x_res):\n",
        "  tokens = word_tokenize(doc)\n",
        "  filtered = [word for word in tokens if word not in stop_words] \n",
        "  #sw_removed_texts.append(\" \".join(filtered))\n",
        "  word_seq.append(filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NzbPJszDehn",
        "outputId": "ff692ce8-9fd0-4bbe-8eb8-ccec950fe8f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 901.76it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_hOWcM8Iwrr",
        "outputId": "faa0f197-4b13-4457-8ada-b6b38f0ac098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1000/1000 [00:01<00:00, 896.66it/s]\n"
          ]
        }
      ],
      "source": [
        "max_seq_len=120\n",
        "MAX_NB_WORDS=100000\n",
        "\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "sw_removed_texts = []\n",
        "for doc in tqdm(x_res):\n",
        "  tokens = word_tokenize(doc)\n",
        "  filtered = [word for word in tokens if word not in stop_words]\n",
        "  sw_removed_texts.append(\" \".join(filtered))\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True, char_level=False)\n",
        "tokenizer.fit_on_texts(sw_removed_texts)  \n",
        "word_seq = tokenizer.texts_to_sequences(sw_removed_texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "word_seq= pad_sequences(word_seq, maxlen=max_seq_len)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9jvuY2pjF7RY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a0aa9d-9ae8-4628-fa53-e3649bc0718f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6162\n",
            "number of null word embeddings: 513\n"
          ]
        }
      ],
      "source": [
        "\n",
        "embed_dim=300\n",
        "words_not_found = []\n",
        "nb_words = min(MAX_NB_WORDS, len(word_index)+1)\n",
        "print(nb_words)\n",
        "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
        "for word, i in word_index.items():\n",
        "  if i >= nb_words:\n",
        "     continue\n",
        "  embedding_vector = embeddings_vectors.get(word)\n",
        "  \n",
        "  if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
        "     embedding_matrix[i] = embedding_vector\n",
        "  else:\n",
        "     words_not_found.append(word)\n",
        "print('number of null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_reproducible(seed):\n",
        "  \n",
        "\n",
        "  os.environ['PYTHONHASHSEED'] = str(0)\n",
        "  # For working on GPUs from \"TensorFlow Determinism\"\n",
        "  os.environ['CUDA_VISBLE_DEVICE'] = ''\n",
        "  #np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  tf.random.set_seed(seed)\n",
        "\n"
      ],
      "metadata": {
        "id": "YoHIaYkntpaS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "wZbZx08WgAg2"
      },
      "outputs": [],
      "source": [
        "def get_model():\n",
        "\n",
        "  \n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(Embedding(nb_words, embed_dim, input_length=max_seq_len, weights=[embedding_matrix],trainable=False))\n",
        "  #model.add(Lambda(lexiconAttention, name=\"lexiconAttention\"))\n",
        "  model.add(tf.keras.layers.LayerNormalization(axis=1)) \n",
        "  model.add(Bidirectional(LSTM(16, return_sequences= True)))\n",
        "  model.add(Bidirectional(LSTM(8, return_sequences= False)))\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "  #model.summary()\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgYTujB2W1hV",
        "outputId": "469e80a9-d67e-4a31-8821-e3d0f1c8d68a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n"
          ]
        }
      ],
      "source": [
        "f_binary_all=[]\n",
        "f_macro_all=[]\n",
        "f_micro_all=[]\n",
        "p_all =[]\n",
        "r_all=[]\n",
        "fold = 0\n",
        "\n",
        "\n",
        "for k in ran:\n",
        "\n",
        "  fold+=1\n",
        "  set_reproducible(k)\n",
        "  x_train_a, x_test_a, y_train_a, y_test_a =train_test_split(word_seq,np.asanyarray(y_res), test_size=0.1, random_state=k)\n",
        "\n",
        "  set_reproducible(k)\n",
        "  model=get_model()\n",
        "  es_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
        "  history = model.fit(x_train_a, y_train_a, batch_size=32, epochs=200,verbose=1, validation_data=(x_test_a, y_test_a), callbacks=[es_callback], shuffle=True)\n",
        "\n",
        "  yhat=model.predict(x_test_a, verbose=0)\n",
        "\n",
        "  yp=[]\n",
        "  for k in range(len(yhat)):\n",
        "    if yhat[k][0]>0.5:\n",
        "      yp.append(1)\n",
        "    else:\n",
        "      yp.append(0)\n",
        "\n",
        "  print(classification_report(y_test_a,yp))   \n",
        "  print('F1-score for fold: %d :' %fold)\n",
        "  f1_macro=f1_score(y_test_a,yp,average='macro')\n",
        "  f1_binary=f1_score(y_test_a,yp,average='binary')\n",
        "  f1_micro=f1_score(y_test_a,yp,average='micro')\n",
        "\n",
        "  print(f1_binary)\n",
        "  print('-----') \n",
        "  f_binary_all.append(f1_binary)\n",
        "\n",
        "  print(f1_macro)\n",
        "  print('-----') \n",
        "  f_macro_all.append(f1_macro)\n",
        "\n",
        "  print(f1_micro)\n",
        "  print('-----') \n",
        "  f_micro_all.append(f1_micro)\n",
        "\n",
        "  print('Recall-score for fold: %d :' %fold)\n",
        "  r=recall_score(y_test_a,yp)\n",
        "  print(r)\n",
        "  print('-----')\n",
        "  print('Precision-score for fold: %d :' %fold)\n",
        "  p=precision_score(y_test_a,yp)\n",
        "  print(p)\n",
        "  print('-----')\n",
        "  print('----------------------------------------------')\n",
        "\n",
        "  r_all.append(r)\n",
        "  p_all.append(p)\n",
        " \n",
        "\n",
        "print(\"Average of F1_binary_Score = {}\".format(np.mean(f_binary_all)))\n",
        "print(\"Standard Deviation of F1_binary_Score = {}\".format(np.std(f_binary_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of F1_macro_Score = {}\".format(np.mean(f_macro_all)))\n",
        "print(\"Standard Deviation of F1_macro_Score = {}\".format(np.std(f_macro_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of F1_micro_Score = {}\".format(np.mean(f_micro_all)))\n",
        "print(\"Standard Deviation of F1_micro_Score = {}\".format(np.std(f_micro_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of Recall_Score = {}\".format(np.mean(r_all)))\n",
        "print(\"Standard Deviation of Recall_Score = {}\".format(np.std(r_all)))\n",
        "print('------')\n",
        "\n",
        "print(\"Average of Precision_Score = {}\".format(np.mean(p_all)))\n",
        "print(\"Standard Deviation of Precision_Score = {}\".format(np.std(p_all)))\n",
        "print('------')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}